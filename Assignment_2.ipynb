{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RqsobDm-U6sP"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Read the content of the file\n",
        "with open('owlcreek.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of tokens\n",
        "num_tokens = len(doc)\n",
        "print(f'Number of tokens: {num_tokens}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJWtTa3QWLCk",
        "outputId": "42de630a-9348-4165-bad0-e7ffdb166823"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 4835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of sentences\n",
        "num_sentences = len(list(doc.sents))\n",
        "print(f'Number of sentences: {num_sentences}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vF3DzgOxWOI6",
        "outputId": "84b9ce16-8945-40f1-ee02-02706ee5391b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences: 204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the second sentence\n",
        "second_sentence = list(doc.sents)[1]\n",
        "print(f'Second sentence: {second_sentence.text}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJ3qIQvWWRB3",
        "outputId": "1bcfee26-b2ed-41b6-8612-b734ddf0ea2b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Second sentence: The man's hands were behind\n",
            "his back, the wrists bound with a cord.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print details for each token in the second sentence\n",
        "for token in second_sentence:\n",
        "    print(f'Text: {token.text}, POS: {token.pos_}, Dep: {token.dep_}, Lemma: {token.lemma_}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBJ0nUVxWYHh",
        "outputId": "71c95c94-1d7a-40bf-a4f6-88c7bba1a5a6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: The, POS: DET, Dep: det, Lemma: the\n",
            "Text: man, POS: NOUN, Dep: poss, Lemma: man\n",
            "Text: 's, POS: PART, Dep: case, Lemma: 's\n",
            "Text: hands, POS: NOUN, Dep: nsubj, Lemma: hand\n",
            "Text: were, POS: AUX, Dep: ROOT, Lemma: be\n",
            "Text: behind, POS: ADP, Dep: prep, Lemma: behind\n",
            "Text: \n",
            ", POS: SPACE, Dep: dep, Lemma: \n",
            "\n",
            "Text: his, POS: PRON, Dep: poss, Lemma: his\n",
            "Text: back, POS: NOUN, Dep: pobj, Lemma: back\n",
            "Text: ,, POS: PUNCT, Dep: punct, Lemma: ,\n",
            "Text: the, POS: DET, Dep: det, Lemma: the\n",
            "Text: wrists, POS: NOUN, Dep: appos, Lemma: wrist\n",
            "Text: bound, POS: VERB, Dep: acl, Lemma: bind\n",
            "Text: with, POS: ADP, Dep: prep, Lemma: with\n",
            "Text: a, POS: DET, Dep: det, Lemma: a\n",
            "Text: cord, POS: NOUN, Dep: pobj, Lemma: cord\n",
            "Text: ., POS: PUNCT, Dep: punct, Lemma: .\n",
            "Text:  , POS: SPACE, Dep: dep, Lemma:  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Initialize the matcher with the shared vocabulary\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define the pattern for \"swimming vigorously\"\n",
        "pattern = [{\"LOWER\": \"swimming\"}, {\"LOWER\": \"vigorously\"}]\n",
        "matcher.add(\"Swimming\", [pattern])\n",
        "\n",
        "# Find matches in the doc\n",
        "matches = matcher(doc)"
      ],
      "metadata": {
        "id": "kR86o_X4WbZN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print surrounding text for each match found\n",
        "for match_id, start, end in matches:\n",
        "    matched_span = doc[start:end]\n",
        "    # Get surrounding context (e.g., 10 characters before and after)\n",
        "    start_context = max(start - 10, 0)\n",
        "    end_context = min(end + 10, len(doc))\n",
        "\n",
        "    surrounding_text = doc[start_context:end_context].text\n",
        "    print(f'Match found: \"{matched_span.text}\"')\n",
        "    print(f'Surrounding text: \"{surrounding_text}\"\\n')"
      ],
      "metadata": {
        "id": "ab3Bo0IBWdfK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fLKtPOrEWgC4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}